═══════════════════════════════════════════════════════════════════════════════
                MACHINE LEARNING - Comprehensive Overview (2025 Edition)
═══════════════════════════════════════════════════════════════════════════════

Last major update concept: January 2025

1. Fundamentals & Mathematics
───────────────────────────────────────────────────────────────────────────────
1.1 Linear Algebra
    • Vectors, matrices, tensors
    • Matrix operations, decompositions (SVD, QR, Cholesky)
    • Eigenvalues/eigenvectors, PCA connection
    • Norms & distances (L1, L2, infinity, Mahalanobis...)

1.2 Probability & Statistics
    • Random variables, distributions (Gaussian, Bernoulli, Categorical, Poisson, Beta, Dirichlet...)
    • Expectation, variance, covariance, correlation
    • Bayes' theorem & Bayesian thinking
    • Maximum Likelihood Estimation (MLE), MAP
    • Bias-variance tradeoff
    • Hypothesis testing, p-values, confidence intervals (still relevant in 2025!)

1.3 Calculus & Optimization
    • Gradients, partial derivatives, chain rule
    • Gradient descent family (Vanilla GD, Momentum, Nesterov, AdaGrad, RMSprop, Adam, AdamW, Lion, Sophia...)
    • Second-order methods (Newton, quasi-Newton, L-BFGS)
    • Learning rate schedules (cosine decay, warmup, Noam, OneCycle...)
    • Constrained & unconstrained optimization

2. Core Machine Learning (Classical ML)
───────────────────────────────────────────────────────────────────────────────
2.1 Supervised Learning – Regression
    • Linear Regression + Ridge + Lasso + ElasticNet
    • Polynomial regression
    • Robust regression (Huber, Quantile)
    • Gaussian Processes (still very strong in small-data + uncertainty)

2.2 Supervised Learning – Classification
    • Logistic Regression
    • Naive Bayes (Gaussian, Multinomial, Complement)
    • Decision Trees → Random Forest → ExtraTrees
    • Gradient Boosting Machines
      - XGBoost
      - LightGBM
      - CatBoost
      - NGBoost (natural gradient + uncertainty)

2.3 Evaluation Metrics (very important!)
    Regression: RMSE, MAE, MAPE, R², MedAE
    Classification: Accuracy, Precision, Recall, F1, Fβ, Matthews Corr. Coef.
    Ranking: NDCG, MAP@k, MRR
    Probabilistic: Log-loss, Brier score, Calibration (ECE, MCE)
    Multi-label & imbalanced: macro/micro/weighted, PR-AUC

2.4 Feature Engineering (still extremely powerful in 2025)
    • Missing value treatment
    • Encoding (target, frequency, WOE, leave-one-out...)
    • Interactions, polynomial features
    • Target encoding with smoothing & noise
    • Feature selection (permutation importance, Boruta, SHAP values)

3. Deep Learning – Building Blocks (2025 status)
───────────────────────────────────────────────────────────────────────────────
3.1 Architectures that really matter (Jan 2025)

    • Transformers (everything is still a transformer...)
      - Encoder-only           → BERT family, DeBERTaV3, RoBERTa, ELECTRA
      - Decoder-only           → GPT-4o, Claude 3.5/4, Grok-2/3, Llama-3.1/4, Qwen2.5, Gemma-2/3, DeepSeek-R1, Mistral-Nemo/Large, Phi-4...
      - Encoder-Decoder        → T5, UL2, Flan-T5, BART, mT5 (still strong for translation/summarization)

    • Vision
      - ViT, Swin Transformer, ConvNeXt V2, EfficientNetV2
      - Vision Transformers 2.0 era: SigLIP, EVA-CLIP, InternViT, DINOv2
      - Diffusion & Flow Matching models (Stable Diffusion 3, Flux.1, SDXL-Turbo, Lumina, PixArt-Σ...)

    • Multimodal & Unified models
      - Chameleon, Flamingo-style, LLaVA-1.6/Next, Qwen-VL, Phi-3.5-vision, InternVL-2, Molmo, Pixtral 12B, Gemma-3 vision...

    • State Space Models (alternative to attention)
      - Mamba-2, MambaByte, Jamba, RWKV-v6/v7, xLSTM

3.2 Training paradigms that dominate 2025

    • Pre-training + Instruction-tuning + Preference Optimization
    • SFT → RLHF → PPO → DPO → IPO → KTO → ORPO → SimPO → RPO...
    • Direct Alignment Algorithms (many labs now prefer over classic RLHF)

    • Continued pre-training (domain adaptation)
    • PEFT methods still very important:
        - LoRA / QLoRA / DoRA
        - AdapterHub / (IA)³
        - LoHa, LoKr, VeRA
        - PiSSA, ReLoRA, LongLoRA

4. Modern ML Practice & Production (2025 reality)
───────────────────────────────────────────────────────────────────────────────
4.1 Data-centric AI
    • Data quality >> model size in many cases
    • Data cleaning, deduplication, filtering (MinHash, SimHash, Data-Juicer...)
    • Synthetic data generation (very strong area)

4.2 Efficient Inference & Serving
    • Quantization: 8-bit → 4-bit → 2-bit → 1.58-bit (ternary)
    • GGUF, AWQ, GPTQ, bitsandbytes, AQLM, QuIP#, E5M2/E4M3 formats
    • Speculative decoding, Medusa, Lookahead, Jacobi, Eagle...
    • vLLM, TGI, SGLang, LMDeploy, TensorRT-LLM, MLC-LLM

4.3 Evaluation in 2025
    • LLM-as-a-Judge (very dominant)
    • Arena / LMSYS Chatbot Arena style blind comparison
    • MT-Bench, AlpacaEval 2, WildBench, Arena-Hard, GPQA, MuSR...
    • New benchmarks: LiveBench, SWE-bench Verified, AgentBench, ToolACE...

5. Advanced & Research Frontiers (early 2026 glimpse)
───────────────────────────────────────────────────────────────────────────────
    • Mixture-of-Experts / MoE scaling (very strong direction)
    • Long-context understanding (128k → 1M → 10M tokens)
    • Reasoning models (o1, o3-mini, DeepSeek-R1, Gemini 2.0 Flash Thinking...)
    • Test-time compute scaling (major trend 2025–2026)
    • World models + video generation + physical reasoning
    • Agentic systems & tool use (very hot)
    • Self-improvement / recursive self-improvement loops
    • Alignment: debate, debate+, constitutional AI, AI safety evals

═══════════════════════════════════════════════════════════════════════════════
          "Machine Learning is 10% math, 20% code, 70% data wrangling."
                    — old joke that is still surprisingly true in 2025
═══════════════════════════════════════════════════════════════════════════════